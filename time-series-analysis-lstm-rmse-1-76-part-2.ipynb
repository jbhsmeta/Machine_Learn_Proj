{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport copy\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #For visualization and plotting Graph\nfrom sklearn.preprocessing import MinMaxScaler # For scaling the data\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential,layers # Layers for building up the LSTM models\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-09T20:11:57.665025Z","iopub.execute_input":"2022-08-09T20:11:57.665447Z","iopub.status.idle":"2022-08-09T20:12:04.106776Z","shell.execute_reply.started":"2022-08-09T20:11:57.665414Z","shell.execute_reply":"2022-08-09T20:12:04.105491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = \"/kaggle/input/stock-time-series-20050101-to-20171231/AAPL_2006-01-01_to_2018-01-01.csv\"","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:08.447952Z","iopub.execute_input":"2022-08-09T20:12:08.449436Z","iopub.status.idle":"2022-08-09T20:12:08.453198Z","shell.execute_reply.started":"2022-08-09T20:12:08.449395Z","shell.execute_reply":"2022-08-09T20:12:08.452412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_init = pd.read_csv(data_path) # Loading the Apple Stock price data\ndf_init.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:09.857054Z","iopub.execute_input":"2022-08-09T20:12:09.857831Z","iopub.status.idle":"2022-08-09T20:12:09.898088Z","shell.execute_reply.started":"2022-08-09T20:12:09.857794Z","shell.execute_reply":"2022-08-09T20:12:09.897359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This method converts the date column to date time object, and set the Date as the index of the data frame.\ndef to_timesr(df_wrk,dt_column, dt_format=None):\n    \n    df=copy.deepcopy(df_wrk) # creating a deep copy of the data, so that the initial dataframe can be kept intact.\n    \n    if dt_format is None:\n        df[dt_column]=pd.to_datetime(df[dt_column],infer_datetime_format=True)\n    else:       \n        df[dt_column]=pd.to_datetime(df[dt_column],format=dt_format)\n        \n    \n    df.set_index(dt_column,inplace=True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:12.743191Z","iopub.execute_input":"2022-08-09T20:12:12.743610Z","iopub.status.idle":"2022-08-09T20:12:12.750828Z","shell.execute_reply.started":"2022-08-09T20:12:12.743577Z","shell.execute_reply":"2022-08-09T20:12:12.749697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Executing the above method\ndf_wrk = to_timesr(df_init,\"Date\",dt_format=None)\ndf_wrk.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:13.287041Z","iopub.execute_input":"2022-08-09T20:12:13.287897Z","iopub.status.idle":"2022-08-09T20:12:13.315040Z","shell.execute_reply.started":"2022-08-09T20:12:13.287838Z","shell.execute_reply":"2022-08-09T20:12:13.314018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Method Visualizing the closing value of the stocks for Apple\ndef plot_stk(df,column,title=None):\n    df[column].plot(kind=\"line\",color='b',figsize=(12,8))\n    plt.title(title)\n    plt.show()\n    \nplot_stk(df_wrk,'Close',title=\"APPLE Stock Closing Value\")","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:16.783299Z","iopub.execute_input":"2022-08-09T20:12:16.784302Z","iopub.status.idle":"2022-08-09T20:12:17.117448Z","shell.execute_reply.started":"2022-08-09T20:12:16.784263Z","shell.execute_reply":"2022-08-09T20:12:17.116354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_scaling(data):\n    ''' This method takes series or dataframe as input and scales the data using MinMaxScaler'''\n    # Initializing the scalar instant\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:21.108075Z","iopub.execute_input":"2022-08-09T20:12:21.109154Z","iopub.status.idle":"2022-08-09T20:12:21.114391Z","shell.execute_reply.started":"2022-08-09T20:12:21.109109Z","shell.execute_reply":"2022-08-09T20:12:21.113245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_data_prep(data, time_step,scaler = None,val_split=None):\n    '''\n    This method aims to scale the data prepare the train data with the help of another method - data_prep_tm_sr(data, time_step); \n    \n    Input:\n    data : dataframe or series-like\n    time_step: no. of obeservations to be fed or the view window\n    Scaler : instance of MinMaxScaler() which will be used for scaling data - Calculated from the module - data_scaling(data):\n    val_split : no.of observation to be kept aside for testing purpose - it can be provided as fraction or as integer\n    \n    Output:\n    x_train_data, y_train_data : training dataset pair\n    val_data_nos : no.of observation in testing data (it is calculated value when val_split is given as fraction)\n    \n    '''    \n    #Generation of the index no. upto which training set will be considered\n    if val_split:\n        if isinstance(val_split,int): #when val split is integer\n            val_data_nos=val_split\n            train_idx_lim=data.shape[0]-val_split\n            #Preparation of training:\n            if scaler:\n                data_temp=data[:train_idx_lim]\n                data_scaled= scaler.transform(data_temp) # only training data is scaled.\n            \n        \n        else:\n            train_idx_lim=int(data.shape[0]*(1-val_split)) #when val_split is fraction\n            val_data_nos=data.shape[0]-train_idx_lim\n        \n            #Preparation of training:\n            if scaler:\n                data_temp=data[0:train_idx_lim]\n                data_scaled= scaler.transform(data_temp) # only training data is scaled.\n    else:\n        data_scaled=scaler.transform(data) #whole series in scaled\n        val_data_nos =0\n        \n    x_train_data,y_train_data=data_prep_tm_sr(data_scaled, time_step)\n    \n        \n    return x_train_data,y_train_data,val_data_nos","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:22.562499Z","iopub.execute_input":"2022-08-09T20:12:22.563374Z","iopub.status.idle":"2022-08-09T20:12:22.573853Z","shell.execute_reply.started":"2022-08-09T20:12:22.563333Z","shell.execute_reply":"2022-08-09T20:12:22.572891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_prep_tm_sr(data, time_step):\n    '''\n    This method aims to prepare the data, as required to be fed to a LSTM.\n    Ex. \n    X=array([[10.68],[10.71],[10.63],[10.9 ],[10.86],[11.55],[11.99],[12.04],[12.23],[12.1 ],[11.78],[11.29],[10.87],[11.1 ],[10.86],[10.6 ],[10.33],[10.29],[10.71],[10.79]])\n    y=[10.77]\n    \n    in X we are taking 20 consecutive values, and the output we are feeding for supervised learning in 10.77\n    For defining the layers of LSTM, 2 inputs are important, Time steps and no.of features, rest no.of batches or no. of observation are basically inferred.\n    Here for our example:\n    Time Step = 20\n    No.of Feature = 1 (Becuase only one feature - \"Close is considered\")\n    The problem framing in this case will be \"many to one\"\n    \n    So this method prepars the data like (many-to-one):\n    X1=values in index (0-19), y = value in 20th index\n    X2 = Values in index (1-20), y = value in 21st index\n    \n    Input:\n    data : dataframe or series-like\n    time_step: no. of obeservations to be fed or the view window\n    val_data_nos : no.of observation in testing data (it is calculated value when val_split is given as fraction)\n        \n    Output:\n    x_data,y_data : training dataset pair\n    \n    '''\n    x_data,y_data=[],[]\n    \n    for i in range(time_step,len(data)):\n            x_data.append(data[i-time_step:i,0])\n            y_data.append(data[i,0])\n            \n    x_data,y_data=np.array(x_data),np.array(y_data)\n    x_data=np.reshape(x_data,(x_data.shape[0],x_data.shape[1],1)) # Reshaping the array as (no.of observations, time_steps, no.of features)\n    \n    return x_data,y_data","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:24.189443Z","iopub.execute_input":"2022-08-09T20:12:24.190601Z","iopub.status.idle":"2022-08-09T20:12:24.198031Z","shell.execute_reply.started":"2022-08-09T20:12:24.190560Z","shell.execute_reply":"2022-08-09T20:12:24.197138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def val_data_prep(data,time_step,val_data_nos,scaler=None):\n    '''\n    This method aims to scale and prepare the validation data.\n    Input:\n    data : dataframe or series like\n    time_step: no. of obeservations to be fed or the view window\n    Scaler : instance of MinMaxScaler() which will be used for scaling data.\n    \n    '''\n    data = data[data.shape[0]-val_data_nos-time_step:]\n    \n    if scaler:\n        data_scaled = scaler.transform(data)    \n        x_val_data,_=data_prep_tm_sr(data_scaled, time_step)\n    else:\n        x_val_data,_=data_prep_tm_sr(data, time_step) #Validation data is prepared in the same way as training data, but the target data is not required in this casesince it will be predicted.\n    \n    return x_val_data","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:25.511299Z","iopub.execute_input":"2022-08-09T20:12:25.511706Z","iopub.status.idle":"2022-08-09T20:12:25.518185Z","shell.execute_reply.started":"2022-08-09T20:12:25.511671Z","shell.execute_reply":"2022-08-09T20:12:25.517066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def model_lstm(input_shape,loss,optimizer):\n    '''\n    This method prepares the LSTM model\n    Input:\n    input_shape: tuple containing time_step and no.of feature, Ex. (20,1)\n    loss: loss function to be used\n    optimizer: optimiser to be used\n    \n    Output:\n    \n    lstm model\n    '''\n    lstm_model=Sequential()\n    lstm_model.add(layers.LSTM(units=50,return_sequences=True,input_shape=input_shape))\n    lstm_model.add(layers.LSTM(units=50))\n    lstm_model.add(layers.Dense(1))\n    \n    lstm_model.compile(loss=loss,optimizer=optimizer)\n    \n    print(lstm_model.summary())\n    \n    return lstm_model","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:26.785160Z","iopub.execute_input":"2022-08-09T20:12:26.785575Z","iopub.status.idle":"2022-08-09T20:12:26.793256Z","shell.execute_reply.started":"2022-08-09T20:12:26.785541Z","shell.execute_reply":"2022-08-09T20:12:26.792239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rmse_loss_pred(valid_data_df):\n    '''\n    This module calculates the rmse loss\n    Input:\n    valid_data_df: dataframe like containing the actual values under column \"Close\" and predicted values under \"Predicted\" column.\n    \n    Output:\n    \n    rmse value\n    \n    '''\n    \n    valid_data_df[\"sqd_err\"] =(valid_data_df[\"Close\"]-valid_data_df[\"Predicted\"])**2\n    rmse = np.sqrt(valid_data_df[\"sqd_err\"].mean())\n    return rmse","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:28.297058Z","iopub.execute_input":"2022-08-09T20:12:28.298000Z","iopub.status.idle":"2022-08-09T20:12:28.303423Z","shell.execute_reply.started":"2022-08-09T20:12:28.297950Z","shell.execute_reply":"2022-08-09T20:12:28.302671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predicted_curve(data,predicted_closing_price,columns):\n    \n    '''\n    This module is to plot the full stock variation against time along with predicted values.\n    Input:\n    data: dataframe like or series - orginal \n    predicted_closing_price : predicted values received from the model\n    columns: string, basically the name of the column in the dataframe to be provided which has the corresponding actual or true value.\n    \n    Output:\n    \n    plot\n        \n    '''\n    val_data_len=len(predicted_closing_price)\n    \n    valid_data_df=data[data.shape[0]-val_data_len:]\n    valid_data_df[\"Predicted\"]=predicted_closing_price\n    \n    #loss Calculation\n    rmse_loss = rmse_loss_pred(valid_data_df) # rmse loss calculation.\n    plt.figure(figsize=(12,8))\n    plt.plot(data[columns])\n    plt.plot(valid_data_df[\"Predicted\"])\n    plt.title(\"Actual vs Prediction Curve\")\n    plt.show()\n    \n    print (\"RMSE Loss : \", rmse_loss)","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:30.132184Z","iopub.execute_input":"2022-08-09T20:12:30.133023Z","iopub.status.idle":"2022-08-09T20:12:30.141278Z","shell.execute_reply.started":"2022-08-09T20:12:30.132974Z","shell.execute_reply":"2022-08-09T20:12:30.140442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def execution(data,time_step,loss,optimizer,epochs,batch_size,verbose,columns,val_split):\n    '''\n    This module executes the compiles and execute all the modules.\n    '''\n    \n    scaler = data_scaling(data)\n    \n    print(\"Preparing training data.....\\n \")\n    x_train_data,y_train_data,val_data_nos = train_data_prep(data, time_step,scaler,val_split)\n    \n    print(\"Preparing validation data.....\\n \")\n    x_val_data = val_data_prep(data,time_step, val_data_nos, scaler)\n    \n    input_shape = (x_train_data.shape[1],x_train_data.shape[2])\n    print(\"Input Shape : \",input_shape,\"\\n\")\n        \n    print(\"Creating the model.....\\n \")\n    model = model_lstm(input_shape,loss,optimizer)\n    \n    callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3) # defining call back, to stop training of there is no significance difference in loss\n    \n    print(\"\\n Fitting the model.....\\n \")\n    history = model.fit(x_train_data,y_train_data,epochs=epochs,batch_size=batch_size,verbose=verbose)    \n           \n    print(\"Predicting the unseen data.....\\n \")\n    predicted_closing_price=model.predict(x_val_data)\n    \n    predicted_closing_price=scaler.inverse_transform(predicted_closing_price)\n    \n    print(\"Printing Actual vs Prediction Curve.....\\n \")\n    plot_predicted_curve(data,predicted_closing_price,columns)","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:34.201749Z","iopub.execute_input":"2022-08-09T20:12:34.202141Z","iopub.status.idle":"2022-08-09T20:12:34.211328Z","shell.execute_reply.started":"2022-08-09T20:12:34.202112Z","shell.execute_reply":"2022-08-09T20:12:34.210574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time_step = 60\nval_split = 251\nloss = \"mean_squared_error\"\noptimizer = \"adam\"\nepochs = 3\nbatch_size = 1\nverbose = 2\ncolumns = \"Close\"","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:37.213695Z","iopub.execute_input":"2022-08-09T20:12:37.214393Z","iopub.status.idle":"2022-08-09T20:12:37.219674Z","shell.execute_reply.started":"2022-08-09T20:12:37.214353Z","shell.execute_reply":"2022-08-09T20:12:37.218646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Condidering only the closing stock price for the current analysis\ndf_wrk_1=df_wrk[[\"Close\"]]","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:37.923901Z","iopub.execute_input":"2022-08-09T20:12:37.925066Z","iopub.status.idle":"2022-08-09T20:12:37.931685Z","shell.execute_reply.started":"2022-08-09T20:12:37.925027Z","shell.execute_reply":"2022-08-09T20:12:37.929693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"execution(df_wrk_1,time_step,loss,optimizer,epochs,batch_size,verbose,columns,val_split)","metadata":{"execution":{"iopub.status.busy":"2022-08-09T20:12:48.587162Z","iopub.execute_input":"2022-08-09T20:12:48.587609Z","iopub.status.idle":"2022-08-09T20:16:14.750336Z","shell.execute_reply.started":"2022-08-09T20:12:48.587576Z","shell.execute_reply":"2022-08-09T20:16:14.748693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}